{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366805c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helpers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile helpers.py\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "def save_experiment_results(\n",
    "    experiment_name,\n",
    "    model_name,\n",
    "    metric_name,\n",
    "    normalization,\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    class_names,\n",
    "    n_train,\n",
    "    n_test,\n",
    "    device,\n",
    "    results_dir=\"/content/drive/MyDrive/pdi/resultados\"\n",
    "):\n",
    "    \"\"\"Salva todos os resultados de um experimento FSL.\"\"\"\n",
    "    \n",
    "    # Criar pasta de resultados\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Calcular N-way e K-shot\n",
    "    num_classes = len(class_names)\n",
    "    shots_per_class = n_train // num_classes\n",
    "    \n",
    "    # Nome do experimento: modelo_metrica_Nway_Kshot\n",
    "    exp_name = f\"{experiment_name}_{num_classes}way_{shots_per_class}shot\"\n",
    "    exp_dir = os.path.join(results_dir, exp_name)\n",
    "    \n",
    "    # Se já existe, adicionar sufixo\n",
    "    counter = 1\n",
    "    original_exp_dir = exp_dir\n",
    "    while os.path.exists(exp_dir):\n",
    "        exp_dir = f\"{original_exp_dir}_v{counter}\"\n",
    "        counter += 1\n",
    "    \n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Calcular acurácia\n",
    "    accuracy = (y_pred == y_true).mean()\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # 1. Gerar relatório detalhado\n",
    "    report_dict = classification_report(\n",
    "        y_true, \n",
    "        y_pred,\n",
    "        target_names=class_names,\n",
    "        digits=3,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    # 2. Salvar configuração do experimento\n",
    "    config = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"n_way\": num_classes,\n",
    "        \"k_shot\": shots_per_class,\n",
    "        \"model\": model_name,\n",
    "        \"metric\": metric_name,\n",
    "        \"normalization\": normalization,\n",
    "        \"n_classes\": num_classes,\n",
    "        \"n_train\": n_train,\n",
    "        \"n_test\": n_test,\n",
    "        \"classes\": class_names,\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # 3. Salvar métricas detalhadas\n",
    "    with open(os.path.join(exp_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(report_dict, f, indent=4)\n",
    "    \n",
    "    # 4. Salvar relatório em texto\n",
    "    with open(os.path.join(exp_dir, \"report.txt\"), \"w\") as f:\n",
    "        f.write(f\"Experimento: {experiment_name}\\n\")\n",
    "        f.write(f\"Data: {timestamp}\\n\")\n",
    "        f.write(f\"{'='*60}\\n\\n\")\n",
    "        f.write(f\"Configuração:\\n\")\n",
    "        f.write(f\"  Modelo: {model_name}\\n\")\n",
    "        f.write(f\"  Métrica: {metric_name}\\n\")\n",
    "        f.write(f\"  Normalização: {normalization}\\n\")\n",
    "        f.write(f\"  N-way: {num_classes} classes\\n\")\n",
    "        f.write(f\"  K-shot: {shots_per_class} exemplos/classe\\n\")\n",
    "        f.write(f\"  Treino: {n_train} imagens ({shots_per_class} por classe)\\n\")\n",
    "        f.write(f\"  Teste: {n_test} imagens\\n\\n\")\n",
    "        f.write(f\"{'='*60}\\n\")\n",
    "        f.write(f\"Acurácia: {accuracy*100:.2f}%\\n\")\n",
    "        f.write(f\"{'='*60}\\n\\n\")\n",
    "        f.write(\"Relatório de Classificação:\\n\\n\")\n",
    "        f.write(classification_report(y_true, y_pred, target_names=class_names, digits=3, zero_division=0))\n",
    "    \n",
    "    # 5. Matriz de Confusão\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.ylabel('Real')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.title(f'{model_name} + {metric_name}\\n{num_classes}-way {shots_per_class}-shot - Acurácia: {accuracy*100:.2f}%')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(exp_dir, \"confusion_matrix.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Salvar predições\n",
    "    predictions_df = {\n",
    "        \"true_label\": y_true.tolist(),\n",
    "        \"predicted_label\": y_pred.tolist(),\n",
    "        \"true_class\": [class_names[i] for i in y_true],\n",
    "        \"predicted_class\": [class_names[i] for i in y_pred],\n",
    "        \"correct\": (y_true == y_pred).tolist()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, \"predictions.json\"), \"w\") as f:\n",
    "        json.dump(predictions_df, f, indent=4)\n",
    "    \n",
    "    # 7. Gráfico de acurácia por classe\n",
    "    per_class_acc = []\n",
    "    for i in range(num_classes):\n",
    "        mask = y_true == i\n",
    "        if mask.sum() > 0:\n",
    "            acc = (y_pred[mask] == y_true[mask]).mean()\n",
    "            per_class_acc.append(acc)\n",
    "        else:\n",
    "            per_class_acc.append(0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(class_names, per_class_acc, color='steelblue')\n",
    "    plt.xlabel('Classe')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.title(f'Acurácia por Classe - {model_name} + {metric_name}\\n{num_classes}-way {shots_per_class}-shot')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(exp_dir, \"accuracy_per_class.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 8. Imprimir resumo\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Resultados salvos em: {exp_dir}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Configuração: {num_classes}-way {shots_per_class}-shot\")\n",
    "    print(f\"Arquivos gerados:\")\n",
    "    print(f\"  - config.json\")\n",
    "    print(f\"  - metrics.json\")\n",
    "    print(f\"  - report.txt\")\n",
    "    print(f\"  - confusion_matrix.png\")\n",
    "    print(f\"  - accuracy_per_class.png\")\n",
    "    print(f\"  - predictions.json\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return exp_dir\n",
    "\n",
    "\n",
    "def save_fewshot_results(\n",
    "    experiment_name,\n",
    "    model_name,\n",
    "    metric_name,\n",
    "    normalization,\n",
    "    accuracies,\n",
    "    n_way,\n",
    "    n_shot,\n",
    "    n_query,\n",
    "    n_episodes,\n",
    "    device,\n",
    "    all_predictions=None,\n",
    "    all_labels=None,\n",
    "    results_dir=\"/content/drive/MyDrive/pdi/resultados\"\n",
    "):\n",
    "    \"\"\"Salva resultados de experimentos few-shot com múltiplos episódios.\"\"\"\n",
    "    \n",
    "    # Criar pasta de resultados\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Nome do experimento\n",
    "    exp_name = f\"{experiment_name}_{n_way}way_{n_shot}shot_{n_episodes}ep\"\n",
    "    exp_dir = os.path.join(results_dir, exp_name)\n",
    "    \n",
    "    # Se já existe, adicionar sufixo\n",
    "    counter = 1\n",
    "    original_exp_dir = exp_dir\n",
    "    while os.path.exists(exp_dir):\n",
    "        exp_dir = f\"{original_exp_dir}_v{counter}\"\n",
    "        counter += 1\n",
    "    \n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Calcular estatísticas\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    min_acc = np.min(accuracies)\n",
    "    max_acc = np.max(accuracies)\n",
    "    median_acc = np.median(accuracies)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Calcular métricas agregadas (se disponível)\n",
    "    metrics_dict = None\n",
    "    if all_predictions is not None and all_labels is not None:\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        all_labels = np.array(all_labels)\n",
    "        \n",
    "        from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "        \n",
    "        precision = precision_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        metrics_dict = {\n",
    "            \"precision_macro\": float(precision),\n",
    "            \"recall_macro\": float(recall),\n",
    "            \"f1_macro\": float(f1)\n",
    "        }\n",
    "    \n",
    "    # 1. Salvar configuração do experimento\n",
    "    config = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"model\": model_name,\n",
    "        \"metric\": metric_name,\n",
    "        \"normalization\": normalization,\n",
    "        \"n_way\": n_way,\n",
    "        \"n_shot\": n_shot,\n",
    "        \"n_query\": n_query,\n",
    "        \"n_episodes\": n_episodes,\n",
    "        \"mean_accuracy\": float(mean_acc),\n",
    "        \"std_accuracy\": float(std_acc),\n",
    "        \"min_accuracy\": float(min_acc),\n",
    "        \"max_accuracy\": float(max_acc),\n",
    "        \"median_accuracy\": float(median_acc),\n",
    "        \"device\": str(device),\n",
    "        \"all_accuracies\": [float(acc) for acc in accuracies]\n",
    "    }\n",
    "    \n",
    "    if metrics_dict:\n",
    "        config.update(metrics_dict)\n",
    "    \n",
    "    with open(os.path.join(exp_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # 2. Salvar relatório em texto\n",
    "    with open(os.path.join(exp_dir, \"report.txt\"), \"w\") as f:\n",
    "        f.write(f\"Experimento: {experiment_name}\\n\")\n",
    "        f.write(f\"Data: {timestamp}\\n\")\n",
    "        f.write(f\"{'='*60}\\n\\n\")\n",
    "        f.write(f\"Configuração:\\n\")\n",
    "        f.write(f\"  Modelo: {model_name}\\n\")\n",
    "        f.write(f\"  Métrica: {metric_name}\\n\")\n",
    "        f.write(f\"  Normalização: {normalization}\\n\")\n",
    "        f.write(f\"  N-way: {n_way} classes por episódio\\n\")\n",
    "        f.write(f\"  K-shot: {n_shot} exemplos/classe\\n\")\n",
    "        f.write(f\"  Query: {n_query} exemplos/classe para teste\\n\")\n",
    "        f.write(f\"  Episódios: {n_episodes}\\n\")\n",
    "        f.write(f\"  Device: {device}\\n\\n\")\n",
    "        f.write(f\"{'='*60}\\n\")\n",
    "        f.write(f\"Resultados:\\n\")\n",
    "        f.write(f\"  Acurácia Média: {mean_acc*100:.2f}% ± {std_acc*100:.2f}%\\n\")\n",
    "        f.write(f\"  Acurácia Mínima: {min_acc*100:.2f}%\\n\")\n",
    "        f.write(f\"  Acurácia Máxima: {max_acc*100:.2f}%\\n\")\n",
    "        f.write(f\"  Mediana: {median_acc*100:.2f}%\\n\")\n",
    "        if metrics_dict:\n",
    "            f.write(f\"\\nMétricas Agregadas (macro):\\n\")\n",
    "            f.write(f\"  Precision: {metrics_dict['precision_macro']*100:.2f}%\\n\")\n",
    "            f.write(f\"  Recall: {metrics_dict['recall_macro']*100:.2f}%\\n\")\n",
    "            f.write(f\"  F1-Score: {metrics_dict['f1_macro']*100:.2f}%\\n\")\n",
    "        f.write(f\"{'='*60}\\n\\n\")\n",
    "        f.write(f\"Acurácias por episódio:\\n\")\n",
    "        for i, acc in enumerate(accuracies, 1):\n",
    "            f.write(f\"  Episódio {i:2d}: {acc*100:.2f}%\\n\")\n",
    "    \n",
    "    # 3. Matriz de Confusão - SEMPRE gera se tiver predições\n",
    "    # Descobrir classes únicas nas predições\n",
    "    unique_labels = np.unique(np.concatenate([all_labels, all_predictions]))\n",
    "    n_classes = len(unique_labels)\n",
    "    \n",
    "    # Nomes das classes (genéricos, já que as classes mudam entre episódios)\n",
    "    class_names_generic = [f\"Classe {i}\" for i in unique_labels]\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_predictions, labels=unique_labels)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names_generic,\n",
    "                yticklabels=class_names_generic)\n",
    "    plt.ylabel('Real', fontsize=12)\n",
    "    plt.xlabel('Predito', fontsize=12)\n",
    "    plt.title(f'{model_name} + {metric_name}\\n{n_way}-way {n_shot}-shot - Acurácia: {mean_acc*100:.2f}% ± {std_acc*100:.2f}%', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(exp_dir, \"confusion_matrix.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Salvar predições\n",
    "    predictions_df = {\n",
    "        \"true_label\": all_labels.tolist(),\n",
    "        \"predicted_label\": all_predictions.tolist(),\n",
    "        \"correct\": (all_labels == all_predictions).tolist()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, \"predictions.json\"), \"w\") as f:\n",
    "        json.dump(predictions_df, f, indent=4)\n",
    "    \n",
    "    # 5. Gráfico: Acurácia por episódio\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    episodes = range(1, n_episodes + 1)\n",
    "    plt.plot(episodes, [acc*100 for acc in accuracies], marker='o', linewidth=2, markersize=6)\n",
    "    plt.axhline(y=mean_acc*100, color='r', linestyle='--', label=f'Média: {mean_acc*100:.2f}%')\n",
    "    plt.fill_between(episodes, \n",
    "                     [(mean_acc - std_acc)*100]*n_episodes, \n",
    "                     [(mean_acc + std_acc)*100]*n_episodes, \n",
    "                     alpha=0.2, color='red', label=f'±1 std: {std_acc*100:.2f}%')\n",
    "    plt.xlabel('Episódio', fontsize=12)\n",
    "    plt.ylabel('Acurácia (%)', fontsize=12)\n",
    "    plt.title(f'{model_name} + {metric_name}\\n{n_way}-way {n_shot}-shot - {n_episodes} episódios', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(exp_dir, \"accuracy_per_episode.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Histograma de acurácias\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(accuracies, bins=15, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    plt.axvline(mean_acc, color='r', linestyle='--', linewidth=2, label=f'Média: {mean_acc*100:.2f}%')\n",
    "    plt.axvline(median_acc, color='g', linestyle='--', linewidth=2, label=f'Mediana: {median_acc*100:.2f}%')\n",
    "    plt.xlabel('Acurácia', fontsize=12)\n",
    "    plt.ylabel('Frequência', fontsize=12)\n",
    "    plt.title(f'Distribuição de Acurácias\\n{model_name} - {n_way}-way {n_shot}-shot', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(exp_dir, \"accuracy_distribution.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 7. Boxplot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.boxplot(accuracies, vert=True, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "                medianprops=dict(color='red', linewidth=2),\n",
    "                whiskerprops=dict(color='blue'),\n",
    "                capprops=dict(color='blue'))\n",
    "    plt.ylabel('Acurácia', fontsize=12)\n",
    "    plt.title(f'Distribuição de Acurácias - Boxplot\\n{model_name} - {n_way}-way {n_shot}-shot', fontsize=14)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(exp_dir, \"accuracy_boxplot.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 8. Imprimir resumo\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Resultados salvos em: {exp_dir}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Configuração: {n_way}-way {n_shot}-shot ({n_episodes} episódios)\")\n",
    "    print(f\"Acurácia: {mean_acc*100:.2f}% ± {std_acc*100:.2f}%\")\n",
    "    if metrics_dict:\n",
    "        print(f\"Precision (macro): {metrics_dict['precision_macro']*100:.2f}%\")\n",
    "        print(f\"Recall (macro): {metrics_dict['recall_macro']*100:.2f}%\")\n",
    "        print(f\"F1-Score (macro): {metrics_dict['f1_macro']*100:.2f}%\")\n",
    "    print(f\"Arquivos gerados:\")\n",
    "    print(f\"  - config.json\")\n",
    "    print(f\"  - report.txt\")\n",
    "    print(f\"  - confusion_matrix.png\")\n",
    "    print(f\"  - predictions.json\")\n",
    "    print(f\"  - accuracy_per_episode.png\")\n",
    "    print(f\"  - accuracy_distribution.png\")\n",
    "    print(f\"  - accuracy_boxplot.png\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return exp_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
