{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee8a608",
   "metadata": {},
   "source": [
    "# Função Auxiliar para Avaliação Few-Shot\n",
    "\n",
    "Função para salvar e visualizar resultados de experimentos de aprendizado few-shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366805c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile helpers.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38a6af",
   "metadata": {},
   "source": [
    "## Função para Experimentos Episódicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fewshot_results(\n",
    "    experiment_name,\n",
    "    model_name,\n",
    "    metric_name,\n",
    "    normalization,\n",
    "    accuracies,\n",
    "    n_way,\n",
    "    n_shot,\n",
    "    n_query,\n",
    "    n_episodes,\n",
    "    device,\n",
    "    all_predictions=None,\n",
    "    all_labels=None,\n",
    "    class_names=None,\n",
    "    results_dir=\"/content/drive/MyDrive/pdi/resultados\"\n",
    "):\n",
    "    \"\"\"Salva resultados de experimentos few-shot com múltiplos episódios.\"\"\"\n",
    "    \n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    exp_name = f\"{experiment_name}_{n_way}way_{n_shot}shot_{n_episodes}ep\"\n",
    "    exp_dir = os.path.join(results_dir, exp_name)\n",
    "    \n",
    "    counter = 1\n",
    "    original_exp_dir = exp_dir\n",
    "    while os.path.exists(exp_dir):\n",
    "        exp_dir = f\"{original_exp_dir}_v{counter}\"\n",
    "        counter += 1\n",
    "    \n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    min_acc = np.min(accuracies)\n",
    "    max_acc = np.max(accuracies)\n",
    "    median_acc = np.median(accuracies)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    metrics_dict = None\n",
    "    report_dict = None\n",
    "    if all_predictions is not None and all_labels is not None:\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        all_labels = np.array(all_labels)\n",
    "        \n",
    "        from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "        \n",
    "        precision = precision_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        metrics_dict = {\n",
    "            \"precision_macro\": float(precision),\n",
    "            \"recall_macro\": float(recall),\n",
    "            \"f1_macro\": float(f1)\n",
    "        }\n",
    "        \n",
    "        if class_names is not None:\n",
    "            report_dict = classification_report(\n",
    "                all_labels,\n",
    "                all_predictions,\n",
    "                target_names=class_names,\n",
    "                digits=4,\n",
    "                output_dict=True,\n",
    "                zero_division=0\n",
    "            )\n",
    "    \n",
    "    # 1. Salvar configuração do experimento\n",
    "    config = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"model\": model_name,\n",
    "        \"metric\": metric_name,\n",
    "        \"normalization\": normalization,\n",
    "        \"n_way\": n_way,\n",
    "        \"n_shot\": n_shot,\n",
    "        \"n_query\": n_query,\n",
    "        \"n_episodes\": n_episodes,\n",
    "        \"mean_accuracy\": float(mean_acc),\n",
    "        \"std_accuracy\": float(std_acc),\n",
    "        \"min_accuracy\": float(min_acc),\n",
    "        \"max_accuracy\": float(max_acc),\n",
    "        \"median_accuracy\": float(median_acc),\n",
    "        \"device\": str(device),\n",
    "        \"all_accuracies\": [float(acc) for acc in accuracies]\n",
    "    }\n",
    "    \n",
    "    if metrics_dict:\n",
    "        config.update(metrics_dict)\n",
    "    \n",
    "    with open(os.path.join(exp_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # 2. Salvar relatório em texto\n",
    "    with open(os.path.join(exp_dir, \"report.txt\"), \"w\") as f:\n",
    "        f.write(f\"Experimento: {experiment_name}\\n\")\n",
    "        f.write(f\"Data: {timestamp}\\n\")\n",
    "        f.write(f\"{'='*60}\\n\\n\")\n",
    "        f.write(f\"Configuração:\\n\")\n",
    "        f.write(f\"  Modelo: {model_name}\\n\")\n",
    "        f.write(f\"  Métrica: {metric_name}\\n\")\n",
    "        f.write(f\"  Normalização: {normalization}\\n\")\n",
    "        f.write(f\"  N-way: {n_way} classes por episódio\\n\")\n",
    "        f.write(f\"  K-shot: {n_shot} exemplos/classe\\n\")\n",
    "        f.write(f\"  Query: {n_query} exemplos/classe para teste\\n\")\n",
    "        f.write(f\"  Episódios: {n_episodes}\\n\")\n",
    "        f.write(f\"  Device: {device}\\n\\n\")\n",
    "        f.write(f\"{'='*60}\\n\")\n",
    "        f.write(f\"Resultados:\\n\")\n",
    "        f.write(f\"  Acurácia Média: {mean_acc*100:.2f}% ± {std_acc*100:.2f}%\\n\")\n",
    "        f.write(f\"  Acurácia Mínima: {min_acc*100:.2f}%\\n\")\n",
    "        f.write(f\"  Acurácia Máxima: {max_acc*100:.2f}%\\n\")\n",
    "        f.write(f\"  Mediana: {median_acc*100:.2f}%\\n\")\n",
    "        if metrics_dict:\n",
    "            f.write(f\"\\nMétricas Agregadas (macro):\\n\")\n",
    "            f.write(f\"  Precision: {metrics_dict['precision_macro']*100:.2f}%\\n\")\n",
    "            f.write(f\"  Recall: {metrics_dict['recall_macro']*100:.2f}%\\n\")\n",
    "            f.write(f\"  F1-Score: {metrics_dict['f1_macro']*100:.2f}%\\n\")\n",
    "        f.write(f\"{'='*60}\\n\\n\")\n",
    "        f.write(f\"Acurácias por episódio:\\n\")\n",
    "        for i, acc in enumerate(accuracies, 1):\n",
    "            f.write(f\"  Episódio {i:2d}: {acc*100:.2f}%\\n\")\n",
    "    \n",
    "    # 3. Salvar relatório detalhado por classe\n",
    "    if report_dict is not None and class_names is not None:\n",
    "        with open(os.path.join(exp_dir, \"per_class_metrics.json\"), \"w\") as f:\n",
    "            json.dump(report_dict, f, indent=4)\n",
    "        \n",
    "        with open(os.path.join(exp_dir, \"per_class_report.txt\"), \"w\") as f:\n",
    "            f.write(f\"{'='*70}\\n\")\n",
    "            f.write(f\"MÉTRICAS DETALHADAS POR CLASSE\\n\")\n",
    "            f.write(f\"{'='*70}\\n\\n\")\n",
    "            f.write(f\"Experimento: {experiment_name}\\n\")\n",
    "            f.write(f\"Data: {timestamp}\\n\")\n",
    "            f.write(f\"Modelo: {model_name}\\n\")\n",
    "            f.write(f\"Configuração: {n_way}-way {n_shot}-shot ({n_episodes} episódios)\\n\\n\")\n",
    "            f.write(f\"{'='*70}\\n\\n\")\n",
    "            \n",
    "            f.write(classification_report(\n",
    "                all_labels,\n",
    "                all_predictions,\n",
    "                target_names=class_names,\n",
    "                digits=4,\n",
    "                zero_division=0\n",
    "            ))\n",
    "            \n",
    "            f.write(f\"\\n{'='*70}\\n\")\n",
    "            f.write(f\"INTERPRETAÇÃO:\\n\")\n",
    "            f.write(f\"{'='*70}\\n\\n\")\n",
    "            f.write(f\"• Precision: De todas as predições de uma classe, quantas estavam corretas\\n\")\n",
    "            f.write(f\"• Recall: De todas as amostras de uma classe, quantas foram identificadas\\n\")\n",
    "            f.write(f\"• F1-Score: Média harmônica entre precision e recall\\n\")\n",
    "            f.write(f\"• Support: Número de amostras reais de cada classe testadas\\n\\n\")\n",
    "            f.write(f\"• macro avg: Média simples das métricas de todas as classes\\n\")\n",
    "            f.write(f\"• weighted avg: Média ponderada pelo suporte de cada classe\\n\\n\")\n",
    "            f.write(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # Gerar tabela visual de métricas\n",
    "        accuracy_overall = (all_predictions == all_labels).mean()\n",
    "        \n",
    "        table_data_classes = []\n",
    "        for class_name in class_names:\n",
    "            metrics = report_dict[class_name]\n",
    "            display_name = class_name.upper() if len(class_name) <= 5 else class_name.title()\n",
    "            table_data_classes.append([\n",
    "                display_name,\n",
    "                f\"{metrics['precision']*100:.0f}\",\n",
    "                f\"{metrics['recall']*100:.0f}\",\n",
    "                f\"{metrics['f1-score']*100:.0f}\",\n",
    "                f\"{int(metrics['support'])}\"\n",
    "            ])\n",
    "        \n",
    "        table_data_summary = [\n",
    "            ['Acurácia (Accuracy)', '-', '-', f\"{accuracy_overall*100:.0f}\", f\"{len(all_labels)}\"],\n",
    "            ['Macro Average', \n",
    "             f\"{report_dict['macro avg']['precision']*100:.0f}\",\n",
    "             f\"{report_dict['macro avg']['recall']*100:.0f}\",\n",
    "             f\"{report_dict['macro avg']['f1-score']*100:.0f}\",\n",
    "             f\"{len(all_labels)}\"],\n",
    "            ['Weighted Average',\n",
    "             f\"{report_dict['weighted avg']['precision']*100:.0f}\",\n",
    "             f\"{report_dict['weighted avg']['recall']*100:.0f}\",\n",
    "             f\"{report_dict['weighted avg']['f1-score']*100:.0f}\",\n",
    "             f\"{len(all_labels)}\"]\n",
    "        ]\n",
    "        \n",
    "        fig = plt.figure(figsize=(11, len(class_names) * 0.55 + 3.5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        col_labels = ['Classe', 'Precision', 'Recall', 'F1-Score', 'Suporte (Imagens)']\n",
    "        \n",
    "        table1 = ax.table(cellText=table_data_classes, colLabels=col_labels,\n",
    "                         cellLoc='center', loc='upper center',\n",
    "                         colWidths=[0.28, 0.16, 0.16, 0.16, 0.24],\n",
    "                         bbox=[0, 0.35, 1, 0.65])\n",
    "        \n",
    "        table1.auto_set_font_size(False)\n",
    "        table1.set_fontsize(12)\n",
    "        \n",
    "        for i in range(len(col_labels)):\n",
    "            cell = table1[(0, i)]\n",
    "            cell.set_facecolor('#E8DCC8')\n",
    "            cell.set_text_props(weight='bold', color='black', fontsize=12)\n",
    "            cell.set_edgecolor('#D3D3D3')\n",
    "            cell.set_linewidth(0.5)\n",
    "        \n",
    "        for i in range(1, len(table_data_classes) + 1):\n",
    "            for j in range(len(col_labels)):\n",
    "                cell = table1[(i, j)]\n",
    "                cell.set_facecolor('#FAF5EF')\n",
    "                cell.set_edgecolor('#D3D3D3')\n",
    "                cell.set_linewidth(0.5)\n",
    "                \n",
    "                if j == 0:\n",
    "                    cell.set_text_props(weight='bold', fontsize=11)\n",
    "                else:\n",
    "                    cell.set_text_props(fontsize=11)\n",
    "        \n",
    "        ax.text(0.5, 0.31, 'MÉDIAS GERAIS', \n",
    "               ha='center', va='center', \n",
    "               fontsize=12, weight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.5', \n",
    "                        facecolor='#E8DCC8', \n",
    "                        edgecolor='#D3D3D3',\n",
    "                        linewidth=0.5))\n",
    "        \n",
    "        table2 = ax.table(cellText=table_data_summary,\n",
    "                         cellLoc='center', loc='lower center',\n",
    "                         colWidths=[0.28, 0.16, 0.16, 0.16, 0.24],\n",
    "                         bbox=[0, 0.0, 1, 0.26])\n",
    "        \n",
    "        table2.auto_set_font_size(False)\n",
    "        table2.set_fontsize(11)\n",
    "        \n",
    "        for i in range(len(table_data_summary)):\n",
    "            for j in range(len(col_labels)):\n",
    "                cell = table2[(i, j)]\n",
    "                cell.set_facecolor('#FAF5EF')\n",
    "                cell.set_edgecolor('#D3D3D3')\n",
    "                cell.set_linewidth(0.5)\n",
    "                \n",
    "                if j == 0:\n",
    "                    cell.set_text_props(weight='bold', fontsize=11)\n",
    "                else:\n",
    "                    cell.set_text_props(fontsize=11)\n",
    "        \n",
    "        plt.savefig(os.path.join(exp_dir, \"metrics_table.png\"), \n",
    "                   dpi=300, bbox_inches='tight', facecolor='white', \n",
    "                   edgecolor='none', pad_inches=0.3)\n",
    "        plt.close()\n",
    "    \n",
    "    # 4. Matriz de confusão\n",
    "    unique_labels = np.unique(np.concatenate([all_labels, all_predictions]))\n",
    "    n_classes = len(unique_labels)\n",
    "    \n",
    "    if class_names is not None:\n",
    "        class_names_for_plot = class_names\n",
    "    else:\n",
    "        class_names_for_plot = [f\"Classe {i}\" for i in unique_labels]\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_predictions, labels=unique_labels)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', \n",
    "                cmap='Blues',\n",
    "                xticklabels=class_names_for_plot,\n",
    "                yticklabels=class_names_for_plot,\n",
    "                annot_kws={'size': 11},\n",
    "                cbar=False,\n",
    "                square=False,\n",
    "                linewidths=0.5,\n",
    "                linecolor='white',\n",
    "                ax=ax)\n",
    "    \n",
    "    ax.set_ylabel('Real', fontsize=11)\n",
    "    ax.set_xlabel('Predito', fontsize=11)\n",
    "    ax.set_title(f'Matriz de Confusão', fontsize=12, pad=12)\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right', fontsize=11)\n",
    "    plt.yticks(rotation=0, fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(exp_dir, \"confusion_matrix.png\"), \n",
    "                dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Salvar predições\n",
    "    predictions_df = {\n",
    "        \"true_label\": all_labels.tolist(),\n",
    "        \"predicted_label\": all_predictions.tolist(),\n",
    "        \"correct\": (all_labels == all_predictions).tolist()\n",
    "    }\n",
    "    \n",
    "    if class_names is not None:\n",
    "        predictions_df[\"true_class\"] = [class_names[int(i)] for i in all_labels]\n",
    "        predictions_df[\"predicted_class\"] = [class_names[int(i)] for i in all_predictions]\n",
    "    \n",
    "    with open(os.path.join(exp_dir, \"predictions.json\"), \"w\") as f:\n",
    "        json.dump(predictions_df, f, indent=4)\n",
    "    \n",
    "    # 6. Gráfico de acurácia por episódio\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    episodes = range(1, n_episodes + 1)\n",
    "    plt.plot(episodes, [acc*100 for acc in accuracies], marker='o', linewidth=2, markersize=4, alpha=0.6)\n",
    "    plt.axhline(y=mean_acc*100, color='r', linestyle='--', linewidth=2, label=f'Média: {mean_acc*100:.2f}%')\n",
    "    plt.fill_between(episodes, \n",
    "                     [(mean_acc - std_acc)*100]*n_episodes, \n",
    "                     [(mean_acc + std_acc)*100]*n_episodes, \n",
    "                     alpha=0.2, color='red', label=f'±1 std: {std_acc*100:.2f}%')\n",
    "    plt.xlabel('Episódio', fontsize=12)\n",
    "    plt.ylabel('Acurácia (%)', fontsize=12)\n",
    "    plt.title(f'{model_name} + {metric_name}\\n{n_way}-way {n_shot}-shot - {n_episodes} episódios', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(exp_dir, \"accuracy_per_episode.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 7. Histograma de acurácias\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(accuracies, bins=15, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    plt.axvline(mean_acc, color='r', linestyle='--', linewidth=2, label=f'Média: {mean_acc*100:.2f}%')\n",
    "    plt.axvline(median_acc, color='g', linestyle='--', linewidth=2, label=f'Mediana: {median_acc*100:.2f}%')\n",
    "    plt.xlabel('Acurácia', fontsize=12)\n",
    "    plt.ylabel('Frequência', fontsize=12)\n",
    "    plt.title(f'Distribuição de Acurácias\\n{model_name} - {n_way}-way {n_shot}-shot', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(exp_dir, \"accuracy_distribution.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 8. Imprimir resumo\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Resultados salvos em: {exp_dir}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Configuração: {n_way}-way {n_shot}-shot ({n_episodes} episódios)\")\n",
    "    print(f\"Acurácia: {mean_acc*100:.2f}% ± {std_acc*100:.2f}%\")\n",
    "    if metrics_dict:\n",
    "        print(f\"Precision (macro): {metrics_dict['precision_macro']*100:.2f}%\")\n",
    "        print(f\"Recall (macro): {metrics_dict['recall_macro']*100:.2f}%\")\n",
    "        print(f\"F1-Score (macro): {metrics_dict['f1_macro']*100:.2f}%\")\n",
    "    print(f\"\\nArquivos gerados:\")\n",
    "    print(f\"  - config.json\")\n",
    "    print(f\"  - report.txt\")\n",
    "    if report_dict is not None:\n",
    "        print(f\"  - per_class_metrics.json\")\n",
    "        print(f\"  - per_class_report.txt\")\n",
    "        print(f\"  - metrics_table.png\")\n",
    "    print(f\"  - confusion_matrix.png\")\n",
    "    print(f\"  - predictions.json\")\n",
    "    print(f\"  - accuracy_per_episode.png\")\n",
    "    print(f\"  - accuracy_distribution.png\")\n",
    "    print(f\"  - accuracy_boxplot.png\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return exp_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
