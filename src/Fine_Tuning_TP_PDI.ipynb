{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixLKpKca5AHA",
        "outputId": "205df891-22b1-4270-de42-26b331506887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Arquivo existe: True\n",
            "Classes usadas: ['nv', 'mel', 'bkl', 'bcc', 'akiec']\n",
            "Separação concluída!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ajustar caminhos para o Drive\n",
        "RAW_PATH = \"/content/drive/MyDrive/Colab_Datasets/skin-cancer-mnist-ham10000\"  # Ajuste conforme sua estrutura\n",
        "OUT_PATH = \"/content/dataset/ham10000\"\n",
        "\n",
        "IMG_DIRS = [\n",
        "    \"HAM10000_images_part_1\",\n",
        "    \"HAM10000_images_part_2\"\n",
        "]\n",
        "\n",
        "META = os.path.join(RAW_PATH, \"HAM10000_metadata.csv\")\n",
        "\n",
        "# Verificar se existe\n",
        "print(f\"Arquivo existe: {os.path.exists(META)}\")\n",
        "\n",
        "# parâmetros few-shot\n",
        "N_CLASSES = 5\n",
        "N_SHOTS = 5\n",
        "N_TEST = 15\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# lê metadata\n",
        "df = pd.read_csv(META)\n",
        "# seleciona classes mais comuns (evita classes raras)\n",
        "classes = df['dx'].value_counts().index[:N_CLASSES]\n",
        "\n",
        "print(\"Classes usadas:\", classes.tolist())\n",
        "\n",
        "# cria pastas\n",
        "for split in ['train', 'test']:\n",
        "    for c in classes:\n",
        "        os.makedirs(f\"{OUT_PATH}/{split}/{c}\", exist_ok=True)\n",
        "\n",
        "# separação\n",
        "for c in classes:\n",
        "    imgs = df[df['dx'] == c]['image_id'].tolist()\n",
        "    random.shuffle(imgs)\n",
        "\n",
        "    train_imgs = imgs[:N_SHOTS]\n",
        "    test_imgs  = imgs[N_SHOTS:N_SHOTS + N_TEST]\n",
        "\n",
        "    for img in train_imgs:\n",
        "        for d in IMG_DIRS:\n",
        "            src = os.path.join(RAW_PATH, d, img + \".jpg\")\n",
        "            if os.path.exists(src):\n",
        "                shutil.copy(src, f\"{OUT_PATH}/train/{c}/\")\n",
        "\n",
        "    for img in test_imgs:\n",
        "        for d in IMG_DIRS:\n",
        "            src = os.path.join(RAW_PATH, d, img + \".jpg\")\n",
        "            if os.path.exists(src):\n",
        "                shutil.copy(src, f\"{OUT_PATH}/test/{c}/\")\n",
        "\n",
        "print(\"Separação concluída!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo"
      ],
      "metadata": {
        "id": "NQNuMIBmGeby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
        "from PIL import Image\n",
        "\n",
        "cudnn.deterministic = False\n",
        "cudnn.benchmark = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "ZaldCrI856MZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class LinearClassifier(nn.Module):\n",
        "    def __init__(self, in_features, num_classes):\n",
        "        super().__init__()\n",
        "        # O baseline supervisionado usa uma projeção linear direta para as classes\n",
        "        self.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "T9sCZQBfdh5l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ],
      "metadata": {
        "id": "wtrYgF3tGDZJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    root=\"/content/dataset/ham10000/train\",\n",
        "    transform=transform_train\n",
        ")\n",
        "\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    root=\"/content/dataset/ham10000/test\",\n",
        "    transform=transform_test\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32, # Batch size padrão (aumentei para 32 para estabilidade do SGD)\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "classes = train_dataset.classes\n",
        "num_classes = len(classes)"
      ],
      "metadata": {
        "id": "UQ5vQpC6GItq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# RegNet (O artigo usa ViT/ResNet, mas a lógica de Fine-tuning é a mesma)\n",
        "model = models.regnet_y_3_2gf(\n",
        "    weights=models.RegNet_Y_3_2GF_Weights.IMAGENET1K_V2\n",
        ")\n",
        "\n",
        "in_features = model.fc.in_features\n",
        "\n",
        "# Substituindo a cabeça pela Linear Layer padrão\n",
        "model.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "GIyZc_y7GLzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d5d5ad-ee75-4af8-9d03-88b57823cb4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/regnet_y_3_2gf-9180c971.pth\" to /root/.cache/torch/hub/checkpoints/regnet_y_3_2gf-9180c971.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 74.6M/74.6M [00:00<00:00, 190MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#função de perda\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    label_smoothing=0.05\n",
        ")"
      ],
      "metadata": {
        "id": "ThuedrtQGTPZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuração para treinamento supervisionado\n",
        "EPOCHS = 10\n",
        "\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "\n",
        "# O artigo usa SGD com momentum 0.9\n",
        "# LR maior na cabeça pq ela é nova\n",
        "optimizer = optim.SGD([\n",
        "    {'params': model.trunk_output.parameters(), 'lr': 1e-5}, # Backbone (LR baixa do artigo)\n",
        "    {'params': model.fc.parameters(), 'lr': 1e-2}            # Cabeça (LR maior para aprender rápido)\n",
        "], momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# Scheduler Cosseno [cite: 195]\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)"
      ],
      "metadata": {
        "id": "t8Cz8HjeGYb3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # treinamento supervisionado\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Métricas\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Atualiza o scheduler ao fim da época\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_acc = 100 * correct / total\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {avg_loss:.4f} Acc: {epoch_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EfXhlY9tcHF",
        "outputId": "fc12dfef-2393-4ed4-83b2-2e2a44e45ceb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 1.6077 Acc: 40.00%\n",
            "Epoch [2/10] Loss: 1.4031 Acc: 44.00%\n",
            "Epoch [3/10] Loss: 1.2593 Acc: 40.00%\n",
            "Epoch [4/10] Loss: 1.0914 Acc: 48.00%\n",
            "Epoch [5/10] Loss: 0.9258 Acc: 64.00%\n",
            "Epoch [6/10] Loss: 0.8100 Acc: 76.00%\n",
            "Epoch [7/10] Loss: 0.8816 Acc: 68.00%\n",
            "Epoch [8/10] Loss: 0.7810 Acc: 84.00%\n",
            "Epoch [9/10] Loss: 0.7346 Acc: 80.00%\n",
            "Epoch [10/10] Loss: 0.8291 Acc: 76.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
        "\n",
        "# \"perform 50 adaptation steps for tasks from novel classes during evaluation phase\"\n",
        "\n",
        "def test_with_adaptation(model, test_loader, n_adaptation_steps=50):\n",
        "    model.eval()\n",
        "\n",
        "    # congela o backbone (pra não destruir o conhecimento base)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # libera apenas a cabeça para treino\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # otimizador\n",
        "    adapt_optimizer = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "    print(f\"Iniciando {n_adaptation_steps} passos de adaptação no teste\")\n",
        "\n",
        "    # pega um batch do teste para treinar 'K-shots'\n",
        "    iter_test = iter(test_loader)\n",
        "    support_images, support_labels = next(iter_test) # pega 1 batch para adaptar\n",
        "    support_images, support_labels = support_images.to(device), support_labels.to(device)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i in range(n_adaptation_steps):\n",
        "        adapt_optimizer.zero_grad()\n",
        "        outputs = model(support_images)\n",
        "        loss = criterion(outputs, support_labels)\n",
        "        loss.backward()\n",
        "        adapt_optimizer.step()\n",
        "\n",
        "    print(\"Adaptação concluída. Avaliando no restante\")\n",
        "\n",
        "    # avaliação real (nas imagens restantes)\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_true.extend(labels.cpu().numpy())\n",
        "\n",
        "    return all_true, all_preds\n",
        "\n",
        "\n",
        "# chamada do teste\n",
        "\n",
        "print(\"Iniciando avaliação com adaptação (Baseline do Artigo)\")\n",
        "\n",
        "# roda a adaptação e pega as predições\n",
        "y_true, y_pred = test_with_adaptation(model, test_loader, n_adaptation_steps=50)\n",
        "\n",
        "# gera as métricas\n",
        "print(f\"\\nBalanced Accuracy: {balanced_accuracy_score(y_true, y_pred)*100:.2f}%\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnSZS-daGaoN",
        "outputId": "96d9365f-5ae3-4ed5-a019-423d705acb71"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando avaliação com adaptação (Baseline do Artigo)...\n",
            "Iniciando 50 passos de adaptação no teste\n",
            "Adaptação concluída. Avaliando no restante\n",
            "\n",
            "Balanced Accuracy: 49.33%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       akiec       0.47      1.00      0.64        15\n",
            "         bcc       0.48      1.00      0.65        15\n",
            "         bkl       0.55      0.40      0.46        15\n",
            "         mel       0.00      0.00      0.00        15\n",
            "          nv       1.00      0.07      0.12        15\n",
            "\n",
            "    accuracy                           0.49        75\n",
            "   macro avg       0.50      0.49      0.38        75\n",
            "weighted avg       0.50      0.49      0.38        75\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}